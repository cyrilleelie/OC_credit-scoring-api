{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9080120f-59c3-44cf-8172-095de3642ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "# Imports pour la modélisation et l'évaluation\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score, recall_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Imports pour le tracking et l'optimisation\n",
    "import mlflow\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "393d1ddf-8891-49b3-80ae-3becb837956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SECTION 1 : FONCTIONS ---\n",
    "\n",
    "def calculate_metrics_and_optimal_threshold(y_true, y_pred_proba, cost_fn=10, cost_fp=1):\n",
    "    \"\"\"\n",
    "    Calcule un ensemble de métriques et trouve le meilleur seuil pour minimiser un coût métier.\n",
    "\n",
    "    Args:\n",
    "        y_true (pd.Series): Les vraies étiquettes (0 ou 1).\n",
    "        y_pred_proba (np.array): Les probabilités prédites pour la classe 1.\n",
    "        cost_fn (int): Coût d'un Faux Négatif.\n",
    "        cost_fp (int): Coût d'un Faux Positif.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un dictionnaire contenant l'AUC, le coût minimum, le meilleur seuil,\n",
    "              et les scores F1 et Recall à ce seuil.\n",
    "    \"\"\"\n",
    "    # Calcul de l'AUC\n",
    "    oof_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "    # Recherche du meilleur seuil pour minimiser le coût métier\n",
    "    thresholds = np.linspace(0.01, 0.99, 100)\n",
    "    costs, f1_scores, recall_scores = [], [], []\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        preds_binary = (y_pred_proba > thresh).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, preds_binary).ravel()\n",
    "        \n",
    "        costs.append((fp * cost_fp) + (fn * cost_fn))\n",
    "        f1_scores.append(f1_score(y_true, preds_binary))\n",
    "        recall_scores.append(recall_score(y_true, preds_binary, pos_label=1))\n",
    "\n",
    "    # Trouver l'index du coût minimum\n",
    "    best_idx = np.argmin(costs)\n",
    "    \n",
    "    # Récupérer les métriques à cet index optimal\n",
    "    metrics = {\n",
    "        'oof_auc': oof_auc,\n",
    "        'min_business_cost': costs[best_idx],\n",
    "        'best_threshold': thresholds[best_idx],\n",
    "        'f1_at_best_threshold': f1_scores[best_idx],\n",
    "        'recall_at_best_threshold': recall_scores[best_idx],\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def train_and_evaluate_model(model_name, model_pipeline, X, y):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle en validation croisée, calcule les métriques et loggue tout dans MLflow.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Le nom à donner à la run MLflow.\n",
    "        model_pipeline: Le modèle ou pipeline scikit-learn à entraîner.\n",
    "        X (pd.DataFrame): Les features.\n",
    "        y (pd.Series): La variable cible.\n",
    "\n",
    "    Returns:\n",
    "        float: Le coût métier minimum pour ce modèle.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        print(f\"\\n--- Démarrage de la run : {model_name} ---\")\n",
    "        mlflow.autolog(disable=True)\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "        # Entraînement en validation croisée\n",
    "        N_SPLITS = 5\n",
    "        skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "        oof_preds = np.zeros(len(X))\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "            print(f\"  Fold {fold + 1}/{N_SPLITS}...\")\n",
    "            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            \n",
    "            model_pipeline.fit(X_train, y_train)\n",
    "            oof_preds[val_idx] = model_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        # Calcul des métriques\n",
    "        all_metrics = calculate_metrics_and_optimal_threshold(y, oof_preds)\n",
    "        \n",
    "        # Logging dans MLflow\n",
    "        mlflow.log_metrics(all_metrics)\n",
    "        print(f\"  Résultats pour {model_name}:\")\n",
    "        for key, value in all_metrics.items():\n",
    "            print(f\"    {key}: {value:.4f}\")\n",
    "            \n",
    "        # Création et sauvegarde du graphique du coût métier\n",
    "        best_threshold_val = all_metrics['best_threshold']\n",
    "        \n",
    "        # Recalculer les coûts pour le graphique\n",
    "        thresholds = np.linspace(0.01, 0.99, 100)\n",
    "        costs = []\n",
    "        cost_fn = 10\n",
    "        cost_fp = 1\n",
    "        for thresh in thresholds:\n",
    "            preds_binary = (oof_preds > thresh).astype(int)\n",
    "            tn, fp, fn, tp = confusion_matrix(y, preds_binary).ravel()\n",
    "            costs.append((fp * cost_fp) + (fn * cost_fn))\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(thresholds, costs)\n",
    "        plt.vlines(best_threshold_val, ymin=min(costs), ymax=max(costs), colors='r', linestyles='--', label=f'Meilleur Seuil ({best_threshold_val:.2f})')\n",
    "        plt.title(f'Coût métier vs Seuil pour {model_name}')\n",
    "        plt.xlabel('Seuil de classification')\n",
    "        plt.ylabel('Coût Métier Total')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Sauvegarder le graphique et le logger comme artefact\n",
    "        plot_filename = f\"{model_name}_cost_threshold.png\"\n",
    "        plt.savefig(plot_filename)\n",
    "        mlflow.log_artifact(plot_filename)\n",
    "        plt.close() # Fermer la figure pour ne pas l'afficher dans le notebook\n",
    "        print(f\"  Graphique du coût métier sauvegardé et loggué comme artefact : {plot_filename}\")\n",
    "\n",
    "        # Sauvegarde du modèle\n",
    "        if \"LogisticRegression\" in model_name or \"Dummy\" in model_name:\n",
    "            mlflow.sklearn.log_model(model_pipeline, f\"{model_name}-pipeline\")\n",
    "        elif \"LightGBM\" in model_name:\n",
    "             mlflow.lightgbm.log_model(model_pipeline, f\"{model_name}-model\")\n",
    "\n",
    "        print(f\"--- Fin de la run : {model_name} ---\")\n",
    "        return all_metrics['min_business_cost']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3525ad5f-42fe-408e-9520-bd2dbf362d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chargement et Préparation des Données ---\n",
      "Chargement des données d'entraînement réussi.\n",
      "Données prêtes avec 796 features.\n"
     ]
    }
   ],
   "source": [
    "# --- SECTION 2 : CHARGEMENT ET PRÉPARATION DES DONNÉES ---\n",
    "\n",
    "print(\"--- Chargement et Préparation des Données ---\")\n",
    "train_df = pd.read_csv('./data/application_train_rdy.csv')\n",
    "print(\"Chargement des données d'entraînement réussi.\")\n",
    "\n",
    "# Remplacer les valeurs infinies par NaN\n",
    "train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Conversion des colonnes 'object' en numérique\n",
    "cols_to_check = [col for col in train_df.columns if col not in ['TARGET', 'SK_ID_CURR']]\n",
    "object_cols = train_df[cols_to_check].select_dtypes(include='object').columns\n",
    "if len(object_cols) > 0:\n",
    "    for col in object_cols:\n",
    "        train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
    "\n",
    "# Préparation finale de X et y\n",
    "TARGET = 'TARGET'\n",
    "y = train_df[TARGET]\n",
    "X = train_df.drop(columns=[TARGET, 'SK_ID_CURR'])\n",
    "X.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in X.columns]\n",
    "print(f\"Données prêtes avec {X.shape[1]} features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c107efaf-8cca-42f0-8189-61e1aafbe37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SECTION 3 : EXPÉRIMENTATIONS DES MODÈLES DE BASE ---\n",
    "EXPERIMENT_NAME = \"Credit Scoring - Comparaison Modeles\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "model_scores = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f99732e-ca18-4e20-9331-c4467e8e1cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Démarrage de la run : Dummy_Classifier_Baseline ---\n",
      "  Fold 1/5...\n",
      "  Fold 2/5...\n",
      "  Fold 3/5...\n",
      "  Fold 4/5...\n",
      "  Fold 5/5...\n",
      "  Résultats pour Dummy_Classifier_Baseline:\n",
      "    oof_auc: 0.5000\n",
      "    min_business_cost: 250986.0000\n",
      "    best_threshold: 0.0100\n",
      "    f1_at_best_threshold: 0.0803\n",
      "    recall_at_best_threshold: 0.0799\n",
      "  Graphique du coût métier sauvegardé et loggué comme artefact : Dummy_Classifier_Baseline_cost_threshold.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/17 12:14:23 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fin de la run : Dummy_Classifier_Baseline ---\n"
     ]
    }
   ],
   "source": [
    "# --- Modèle 1 : Dummy Classifier ---\n",
    "dummy_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('classifier', DummyClassifier(strategy='stratified', random_state=42))\n",
    "])\n",
    "model_scores['Dummy_Classifier'] = train_and_evaluate_model(\"Dummy_Classifier_Baseline\", dummy_pipeline, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e30adb42-e5f2-4f5c-9d59-2bd9795f9760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Démarrage de la run : Logistic_Regression_Baseline ---\n",
      "  Fold 1/5...\n",
      "  Fold 2/5...\n",
      "  Fold 3/5...\n",
      "  Fold 4/5...\n",
      "  Fold 5/5...\n",
      "  Résultats pour Logistic_Regression_Baseline:\n",
      "    oof_auc: 0.7727\n",
      "    min_business_cost: 156040.0000\n",
      "    best_threshold: 0.5247\n",
      "    f1_at_best_threshold: 0.2882\n",
      "    recall_at_best_threshold: 0.6685\n",
      "  Graphique du coût métier sauvegardé et loggué comme artefact : Logistic_Regression_Baseline_cost_threshold.png\n",
      "--- Fin de la run : Logistic_Regression_Baseline ---\n"
     ]
    }
   ],
   "source": [
    "# --- Modèle 2 : Régression Logistique ---\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(class_weight='balanced', random_state=42, solver='liblinear'))\n",
    "])\n",
    "model_scores['Logistic_Regression'] = train_and_evaluate_model(\"Logistic_Regression_Baseline\", logreg_pipeline, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f124507a-b203-410f-a98c-fbdbbba977c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Démarrage de la run : LightGBM_Baseline ---\n",
      "  Fold 1/5...\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226145\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.408473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98871\n",
      "[LightGBM] [Info] Number of data points in the train set: 246005, number of used features: 728\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "  Fold 2/5...\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226145\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.755328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 99035\n",
      "[LightGBM] [Info] Number of data points in the train set: 246005, number of used features: 730\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "  Fold 3/5...\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.748460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98949\n",
      "[LightGBM] [Info] Number of data points in the train set: 246006, number of used features: 727\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432473\n",
      "[LightGBM] [Info] Start training from score -2.432473\n",
      "  Fold 4/5...\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.753997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 99012\n",
      "[LightGBM] [Info] Number of data points in the train set: 246006, number of used features: 729\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432473\n",
      "[LightGBM] [Info] Start training from score -2.432473\n",
      "  Fold 5/5...\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.744990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 99007\n",
      "[LightGBM] [Info] Number of data points in the train set: 246006, number of used features: 726\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432473\n",
      "[LightGBM] [Info] Start training from score -2.432473\n",
      "  Résultats pour LightGBM_Baseline:\n",
      "    oof_auc: 0.7819\n",
      "    min_business_cost: 153218.0000\n",
      "    best_threshold: 0.0892\n",
      "    f1_at_best_threshold: 0.2983\n",
      "    recall_at_best_threshold: 0.6585\n",
      "  Graphique du coût métier sauvegardé et loggué comme artefact : LightGBM_Baseline_cost_threshold.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/17 14:42:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fin de la run : LightGBM_Baseline ---\n"
     ]
    }
   ],
   "source": [
    "# --- Modèle 3 : LightGBM ---\n",
    "# Pour LightGBM, on impute les NaN mais on ne scale pas les données.\n",
    "lgbm_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('classifier', lgb.LGBMClassifier(random_state=42))\n",
    "])\n",
    "model_scores['LightGBM'] = train_and_evaluate_model(\"LightGBM_Baseline\", lgbm_pipeline, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59007fd9-9cf8-45d3-b211-87d282ec0377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Meilleur modèle de base sélectionné : LightGBM (Coût: 153218) ---\n"
     ]
    }
   ],
   "source": [
    "# --- SECTION 4 : SÉLECTION ET OPTIMISATION DU MEILLEUR MODÈLE ---\n",
    "\n",
    "# Sélection du meilleur modèle basé sur le coût métier le plus bas\n",
    "best_model_name = min(model_scores, key=model_scores.get)\n",
    "print(f\"\\n--- Meilleur modèle de base sélectionné : {best_model_name} (Coût: {model_scores[best_model_name]}) ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "640888da-2aff-4be6-9193-d66079322c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 15:55:24,740] A new study created in memory with name: no-name-4dc581e1-7cb5-460d-8f9f-3753a9433de2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Lancement de l'optimisation des hyperparamètres pour LightGBM avec Optuna ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 16:05:31,256] Trial 0 finished with value: 152246.0 and parameters: {'learning_rate': 0.012645836252328598, 'num_leaves': 97, 'max_depth': 12, 'min_child_samples': 133, 'subsample': 0.7934636795299609, 'colsample_bytree': 0.8559154146745591}. Best is trial 0 with value: 152246.0.\n",
      "[I 2025-06-17 16:10:24,909] Trial 1 finished with value: 153667.0 and parameters: {'learning_rate': 0.0747774484504541, 'num_leaves': 300, 'max_depth': 9, 'min_child_samples': 182, 'subsample': 0.9846459514304582, 'colsample_bytree': 0.8929942723073518}. Best is trial 0 with value: 152246.0.\n",
      "[I 2025-06-17 16:16:46,904] Trial 2 finished with value: 152154.0 and parameters: {'learning_rate': 0.02520389388297912, 'num_leaves': 140, 'max_depth': 10, 'min_child_samples': 125, 'subsample': 0.7559407223181028, 'colsample_bytree': 0.7374614193982663}. Best is trial 2 with value: 152154.0.\n",
      "[I 2025-06-17 16:21:18,812] Trial 3 finished with value: 153930.0 and parameters: {'learning_rate': 0.0719396693069315, 'num_leaves': 268, 'max_depth': 8, 'min_child_samples': 120, 'subsample': 0.7312135538258421, 'colsample_bytree': 0.7846792110520575}. Best is trial 2 with value: 152154.0.\n",
      "[I 2025-06-17 16:28:22,583] Trial 4 finished with value: 153024.0 and parameters: {'learning_rate': 0.02092700515613637, 'num_leaves': 85, 'max_depth': 10, 'min_child_samples': 137, 'subsample': 0.6878392434684439, 'colsample_bytree': 0.9806427788409109}. Best is trial 2 with value: 152154.0.\n",
      "[I 2025-06-17 16:33:35,914] Trial 5 finished with value: 155886.0 and parameters: {'learning_rate': 0.028740570428465475, 'num_leaves': 238, 'max_depth': 6, 'min_child_samples': 65, 'subsample': 0.9705790553700688, 'colsample_bytree': 0.9445122891663256}. Best is trial 2 with value: 152154.0.\n",
      "[I 2025-06-17 16:40:38,220] Trial 6 finished with value: 155422.0 and parameters: {'learning_rate': 0.013030907220215158, 'num_leaves': 120, 'max_depth': 7, 'min_child_samples': 72, 'subsample': 0.6848945831373295, 'colsample_bytree': 0.8754951634234855}. Best is trial 2 with value: 152154.0.\n",
      "[I 2025-06-17 16:47:45,018] Trial 7 finished with value: 154057.0 and parameters: {'learning_rate': 0.017371818447066915, 'num_leaves': 94, 'max_depth': 9, 'min_child_samples': 79, 'subsample': 0.7386202344838537, 'colsample_bytree': 0.7995458826970695}. Best is trial 2 with value: 152154.0.\n",
      "[I 2025-06-17 16:53:35,709] Trial 8 finished with value: 152335.0 and parameters: {'learning_rate': 0.046507072622475956, 'num_leaves': 254, 'max_depth': 10, 'min_child_samples': 130, 'subsample': 0.9915853356187102, 'colsample_bytree': 0.6537841349714854}. Best is trial 2 with value: 152154.0.\n",
      "[I 2025-06-17 16:59:02,049] Trial 9 finished with value: 154052.0 and parameters: {'learning_rate': 0.09071695903369403, 'num_leaves': 156, 'max_depth': 11, 'min_child_samples': 174, 'subsample': 0.9775931685187407, 'colsample_bytree': 0.9299144721850404}. Best is trial 2 with value: 152154.0.\n",
      "[I 2025-06-17 17:03:09,455] Trial 10 finished with value: 155927.0 and parameters: {'learning_rate': 0.038951785803837395, 'num_leaves': 38, 'max_depth': 3, 'min_child_samples': 21, 'subsample': 0.6102176363165277, 'colsample_bytree': 0.683956364354806}. Best is trial 2 with value: 152154.0.\n",
      "[I 2025-06-17 17:14:32,075] Trial 11 finished with value: 151440.0 and parameters: {'learning_rate': 0.011325314745910314, 'num_leaves': 178, 'max_depth': 12, 'min_child_samples': 158, 'subsample': 0.8568410806799079, 'colsample_bytree': 0.733364746321699}. Best is trial 11 with value: 151440.0.\n",
      "[I 2025-06-17 17:26:46,634] Trial 12 finished with value: 151574.0 and parameters: {'learning_rate': 0.010029842970620602, 'num_leaves': 189, 'max_depth': 12, 'min_child_samples': 162, 'subsample': 0.8571898235357694, 'colsample_bytree': 0.7204006521413893}. Best is trial 11 with value: 151440.0.\n",
      "[I 2025-06-17 17:37:32,553] Trial 13 finished with value: 151258.0 and parameters: {'learning_rate': 0.010241101044512771, 'num_leaves': 204, 'max_depth': 12, 'min_child_samples': 200, 'subsample': 0.8843947846459445, 'colsample_bytree': 0.7174115065647507}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 17:42:56,489] Trial 14 finished with value: 153730.0 and parameters: {'learning_rate': 0.010230192630453171, 'num_leaves': 202, 'max_depth': 5, 'min_child_samples': 195, 'subsample': 0.9010725131324201, 'colsample_bytree': 0.6124939469764512}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 17:50:45,628] Trial 15 finished with value: 152435.0 and parameters: {'learning_rate': 0.01504383641706153, 'num_leaves': 199, 'max_depth': 12, 'min_child_samples': 157, 'subsample': 0.8745028011079516, 'colsample_bytree': 0.7499842373769179}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 17:57:25,486] Trial 16 finished with value: 152343.0 and parameters: {'learning_rate': 0.019646860210921234, 'num_leaves': 223, 'max_depth': 11, 'min_child_samples': 186, 'subsample': 0.9120356354849791, 'colsample_bytree': 0.6864550362364674}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 18:02:20,718] Trial 17 finished with value: 154015.0 and parameters: {'learning_rate': 0.013178022044486941, 'num_leaves': 169, 'max_depth': 3, 'min_child_samples': 198, 'subsample': 0.8284851680635894, 'colsample_bytree': 0.6003342985489993}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 18:07:38,866] Trial 18 finished with value: 153093.0 and parameters: {'learning_rate': 0.037548685701655767, 'num_leaves': 171, 'max_depth': 11, 'min_child_samples': 150, 'subsample': 0.9296980462678327, 'colsample_bytree': 0.8360784765287315}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 18:13:12,833] Trial 19 finished with value: 152312.0 and parameters: {'learning_rate': 0.022778327345526837, 'num_leaves': 220, 'max_depth': 8, 'min_child_samples': 108, 'subsample': 0.8359577621659343, 'colsample_bytree': 0.7714682416076221}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 18:17:33,486] Trial 20 finished with value: 155653.0 and parameters: {'learning_rate': 0.016280561309797027, 'num_leaves': 48, 'max_depth': 5, 'min_child_samples': 97, 'subsample': 0.7890781876382634, 'colsample_bytree': 0.6969535607057563}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 18:26:50,397] Trial 21 finished with value: 151863.0 and parameters: {'learning_rate': 0.010623812038451481, 'num_leaves': 190, 'max_depth': 12, 'min_child_samples': 166, 'subsample': 0.861346817362572, 'colsample_bytree': 0.7270538859591952}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 18:35:29,733] Trial 22 finished with value: 152356.0 and parameters: {'learning_rate': 0.010166328073780364, 'num_leaves': 138, 'max_depth': 12, 'min_child_samples': 151, 'subsample': 0.86768618458938, 'colsample_bytree': 0.7183106543844225}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 18:43:47,119] Trial 23 finished with value: 151263.0 and parameters: {'learning_rate': 0.011970750525575391, 'num_leaves': 184, 'max_depth': 11, 'min_child_samples': 167, 'subsample': 0.9366906713988429, 'colsample_bytree': 0.66219298920643}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 18:51:38,794] Trial 24 finished with value: 152048.0 and parameters: {'learning_rate': 0.012881779633225362, 'num_leaves': 220, 'max_depth': 11, 'min_child_samples': 200, 'subsample': 0.9465344673807473, 'colsample_bytree': 0.6380354632696729}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 18:57:55,726] Trial 25 finished with value: 152356.0 and parameters: {'learning_rate': 0.015457510726014135, 'num_leaves': 156, 'max_depth': 9, 'min_child_samples': 176, 'subsample': 0.9059742080562511, 'colsample_bytree': 0.6501721007579372}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 19:05:01,911] Trial 26 finished with value: 152371.0 and parameters: {'learning_rate': 0.018053617328117083, 'num_leaves': 275, 'max_depth': 10, 'min_child_samples': 147, 'subsample': 0.9393277658713636, 'colsample_bytree': 0.7659302605671847}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 19:12:30,503] Trial 27 finished with value: 152171.0 and parameters: {'learning_rate': 0.01206015500377416, 'num_leaves': 133, 'max_depth': 11, 'min_child_samples': 171, 'subsample': 0.8250513643418752, 'colsample_bytree': 0.6692460087458302}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 19:17:39,758] Trial 28 finished with value: 152735.0 and parameters: {'learning_rate': 0.050424625841106324, 'num_leaves': 177, 'max_depth': 12, 'min_child_samples': 186, 'subsample': 0.8863428317742448, 'colsample_bytree': 0.8287635757696267}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 19:24:12,894] Trial 29 finished with value: 152039.0 and parameters: {'learning_rate': 0.014246889809502955, 'num_leaves': 112, 'max_depth': 12, 'min_child_samples': 140, 'subsample': 0.8008272054785184, 'colsample_bytree': 0.6271590253970604}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 19:31:53,012] Trial 30 finished with value: 155376.0 and parameters: {'learning_rate': 0.011688212084332392, 'num_leaves': 244, 'max_depth': 11, 'min_child_samples': 31, 'subsample': 0.9468871737882014, 'colsample_bytree': 0.8188111275266162}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 19:40:44,133] Trial 31 finished with value: 151625.0 and parameters: {'learning_rate': 0.011417900585945068, 'num_leaves': 187, 'max_depth': 12, 'min_child_samples': 164, 'subsample': 0.8565613411976436, 'colsample_bytree': 0.7091278460951351}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 19:50:35,972] Trial 32 finished with value: 151664.0 and parameters: {'learning_rate': 0.010387886733406473, 'num_leaves': 208, 'max_depth': 12, 'min_child_samples': 162, 'subsample': 0.8016273076913842, 'colsample_bytree': 0.7478346267776415}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 19:58:13,421] Trial 33 finished with value: 151780.0 and parameters: {'learning_rate': 0.013639709231832565, 'num_leaves': 156, 'max_depth': 10, 'min_child_samples': 184, 'subsample': 0.8475166665752246, 'colsample_bytree': 0.6738648726212841}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 20:08:05,417] Trial 34 finished with value: 152058.0 and parameters: {'learning_rate': 0.011970325183067348, 'num_leaves': 181, 'max_depth': 11, 'min_child_samples': 122, 'subsample': 0.9135709677067662, 'colsample_bytree': 0.7200226991523695}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 20:15:30,736] Trial 35 finished with value: 152518.0 and parameters: {'learning_rate': 0.02589916412308883, 'num_leaves': 207, 'max_depth': 12, 'min_child_samples': 188, 'subsample': 0.8889413545943231, 'colsample_bytree': 0.7458403550933901}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 20:26:21,244] Trial 36 finished with value: 152803.0 and parameters: {'learning_rate': 0.010057788134705864, 'num_leaves': 229, 'max_depth': 9, 'min_child_samples': 139, 'subsample': 0.7835806406827212, 'colsample_bytree': 0.7774486690886774}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 20:33:59,267] Trial 37 finished with value: 153917.0 and parameters: {'learning_rate': 0.01921876710117737, 'num_leaves': 275, 'max_depth': 10, 'min_child_samples': 111, 'subsample': 0.8132942471413929, 'colsample_bytree': 0.6990274522424277}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 20:41:22,533] Trial 38 finished with value: 152580.0 and parameters: {'learning_rate': 0.014326926915657711, 'num_leaves': 72, 'max_depth': 11, 'min_child_samples': 178, 'subsample': 0.9584138652093049, 'colsample_bytree': 0.798010058869394}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 20:46:25,269] Trial 39 finished with value: 154103.0 and parameters: {'learning_rate': 0.022521279745573303, 'num_leaves': 256, 'max_depth': 7, 'min_child_samples': 159, 'subsample': 0.8811029372045296, 'colsample_bytree': 0.667249666323734}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 20:55:09,157] Trial 40 finished with value: 151576.0 and parameters: {'learning_rate': 0.016175573364933672, 'num_leaves': 290, 'max_depth': 10, 'min_child_samples': 144, 'subsample': 0.7692706014325652, 'colsample_bytree': 0.7325465446368368}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 21:04:51,911] Trial 41 finished with value: 152158.0 and parameters: {'learning_rate': 0.011492308520652207, 'num_leaves': 289, 'max_depth': 10, 'min_child_samples': 131, 'subsample': 0.7718961151356585, 'colsample_bytree': 0.7245535423535346}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 21:13:47,367] Trial 42 finished with value: 151990.0 and parameters: {'learning_rate': 0.016827250721034335, 'num_leaves': 297, 'max_depth': 12, 'min_child_samples': 145, 'subsample': 0.704071705204573, 'colsample_bytree': 0.7575134981418418}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 21:22:14,946] Trial 43 finished with value: 151734.0 and parameters: {'learning_rate': 0.012920523051778408, 'num_leaves': 148, 'max_depth': 11, 'min_child_samples': 171, 'subsample': 0.7528362164292592, 'colsample_bytree': 0.7405455046458697}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 21:30:42,577] Trial 44 finished with value: 152844.0 and parameters: {'learning_rate': 0.011341001354721564, 'num_leaves': 241, 'max_depth': 10, 'min_child_samples': 152, 'subsample': 0.8438042038177758, 'colsample_bytree': 0.69263826519079}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 21:37:10,690] Trial 45 finished with value: 153011.0 and parameters: {'learning_rate': 0.014125858139442197, 'num_leaves': 116, 'max_depth': 9, 'min_child_samples': 115, 'subsample': 0.715474904265262, 'colsample_bytree': 0.6545504457791677}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 21:42:59,611] Trial 46 finished with value: 152837.0 and parameters: {'learning_rate': 0.03313634784353613, 'num_leaves': 191, 'max_depth': 11, 'min_child_samples': 99, 'subsample': 0.9226703813562012, 'colsample_bytree': 0.788107987233026}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 21:47:37,999] Trial 47 finished with value: 154282.0 and parameters: {'learning_rate': 0.06693507355273932, 'num_leaves': 169, 'max_depth': 12, 'min_child_samples': 191, 'subsample': 0.6572812096948206, 'colsample_bytree': 0.7342770879191638}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 21:53:56,125] Trial 48 finished with value: 153409.0 and parameters: {'learning_rate': 0.015292140717009605, 'num_leaves': 132, 'max_depth': 8, 'min_child_samples': 131, 'subsample': 0.7602806667893858, 'colsample_bytree': 0.7032714191061108}. Best is trial 13 with value: 151258.0.\n",
      "[I 2025-06-17 22:01:44,461] Trial 49 finished with value: 152588.0 and parameters: {'learning_rate': 0.012618390019504868, 'num_leaves': 256, 'max_depth': 10, 'min_child_samples': 156, 'subsample': 0.9747702238519325, 'colsample_bytree': 0.6257591116060144}. Best is trial 13 with value: 151258.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimisation terminée ! Logging des meilleurs résultats...\n",
      "Run de résumé des meilleurs résultats créée avec succès dans la run parente.\n",
      "\n",
      "--- Processus terminé ---\n"
     ]
    }
   ],
   "source": [
    "if \"LightGBM\" in best_model_name:\n",
    "    print(\"\\n--- Lancement de l'optimisation des hyperparamètres pour LightGBM avec Optuna ---\")\n",
    "    \n",
    "    # MODIFIÉ : la fonction objective gère maintenant le logging MLflow\n",
    "    def lgbm_objective(trial):\n",
    "        with mlflow.start_run(run_name=f\"LightGBM_optuna_trial_{trial.number}\", nested=True):\n",
    "            params = {\n",
    "                'objective': 'binary', 'metric': 'auc', 'verbose': -1, 'n_jobs': -1, 'seed': 42,\n",
    "                'n_estimators': 2000,\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 20, 200),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            }\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "            pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')), ('classifier', model)])\n",
    "\n",
    "            N_SPLITS = 5\n",
    "            skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "            oof_preds = np.zeros(len(X))\n",
    "            for train_idx, val_idx in skf.split(X, y):\n",
    "                X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "                X_val = X.iloc[val_idx]\n",
    "                callbacks = [lgb.early_stopping(100, verbose=False)]\n",
    "                pipeline.fit(X_train, y_train, classifier__callbacks=callbacks, classifier__eval_set=[(X_val, y.iloc[val_idx])])\n",
    "                oof_preds[val_idx] = pipeline.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            metrics = calculate_metrics_and_optimal_threshold(y, oof_preds)\n",
    "            mlflow.log_metrics(metrics)\n",
    "            \n",
    "            return metrics['min_business_cost']\n",
    "\n",
    "    # On crée une run parente pour englober toute l'étude d'optimisation\n",
    "    with mlflow.start_run(run_name=\"LightGBM_Optuna_Parent_Run\"):\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        # MODIFIÉ : On enlève le callback, le logging est maintenant manuel\n",
    "        study.optimize(lgbm_objective, n_trials=50)\n",
    "\n",
    "        # Logging des meilleurs résultats dans la run parente\n",
    "        print(\"\\nOptimisation terminée ! Logging des meilleurs résultats...\")\n",
    "        best_trial = study.best_trial\n",
    "        mlflow.log_params(best_trial.params)\n",
    "        mlflow.log_metric(\"best_business_cost_overall\", best_trial.value)\n",
    "        print(\"Run de résumé des meilleurs résultats créée avec succès dans la run parente.\")\n",
    "\n",
    "print(\"\\n--- Processus terminé ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f93cce-a8b8-4937-be13-03c4a14aa7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Chargement et Préparation des Données ---\n",
      "Chargement des données train/test réussi.\n",
      "Données prêtes avec 796 features.\n",
      "\n",
      "--- 2. Entraînement du Modèle Final ---\n",
      "Modèle final entraîné.\n",
      "\n",
      "--- 3. Analyse de l'Importance des Features ---\n",
      "  - Calcul de l'importance globale des features...\n",
      "  - Graphique de l'importance globale loggué comme artefact.\n",
      "  - Calcul de l'importance locale (SHAP)...\n",
      "  - Graphique résumé SHAP loggué comme artefact.\n",
      "  - Création des graphiques waterfall SHAP pour 3 clients aléatoires...\n",
      "    - Waterfall plot pour le client 208550 loggué comme artefact.\n",
      "    - Waterfall plot pour le client 173779 loggué comme artefact.\n",
      "    - Waterfall plot pour le client 365820 loggué comme artefact.\n",
      "\n",
      "--- 4. Enregistrement du Modèle dans le Model Registry ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/23 09:45:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle enregistré dans le Model Registry sous le nom : 'CreditScoringModel'\n",
      "\n",
      "--- Test du Serving en chargeant le modèle depuis le registre ---\n",
      "Modèle chargé avec succès depuis le registre.\n",
      "\n",
      "Prédiction test pour le client 208550:\n",
      "  Probabilité (classe 1) : 0.0000\n",
      "\n",
      "--- Processus Final Terminé ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'CreditScoringModel' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'CreditScoringModel'.\n"
     ]
    }
   ],
   "source": [
    "# --- SECTION 1 : CHARGEMENT ET PRÉPARATION DES DONNÉES ---\n",
    "print(\"--- 1. Chargement et Préparation des Données ---\")\n",
    "train_df = pd.read_csv('./data/application_train_rdy.csv')\n",
    "test_df = pd.read_csv('./data/application_test_rdy.csv')\n",
    "print(\"Chargement des données train/test réussi.\")\n",
    "\n",
    "if not train_df.empty:\n",
    "    # Appliquer les mêmes étapes de nettoyage aux deux jeux de données\n",
    "    for df in [train_df, test_df]:\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        cols_to_check = [col for col in df.columns if col not in ['TARGET', 'SK_ID_CURR']]\n",
    "        object_cols = df[cols_to_check].select_dtypes(include='object').columns\n",
    "        if len(object_cols) > 0:\n",
    "            for col in object_cols:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Préparation finale de X, y et X_test\n",
    "    TARGET = 'TARGET'\n",
    "    y = train_df[TARGET]\n",
    "    \n",
    "    train_ids = train_df['SK_ID_CURR']\n",
    "    test_ids = test_df['SK_ID_CURR']\n",
    "    \n",
    "    # On enlève la cible et l'ID du jeu d'entraînement\n",
    "    X = train_df.drop(columns=[TARGET, 'SK_ID_CURR'])\n",
    "    \n",
    "    # On enlève les mêmes colonnes du jeu de test\n",
    "    cols_to_drop_from_test = ['SK_ID_CURR']\n",
    "    if TARGET in test_df.columns:\n",
    "        cols_to_drop_from_test.append(TARGET)\n",
    "    X_test = test_df.drop(columns=cols_to_drop_from_test)\n",
    "\n",
    "    # Nettoyage des noms de colonnes\n",
    "    X.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in X.columns]\n",
    "    X_test.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in X_test.columns]\n",
    "    \n",
    "    print(f\"Données prêtes avec {X.shape[1]} features.\")\n",
    "\n",
    "    # --- SECTION 2 : ENTRAÎNEMENT DU MODÈLE FINAL ---\n",
    "    print(\"\\n--- 2. Entraînement du Modèle Final ---\")\n",
    "\n",
    "    # !! IMPORTANT !!\n",
    "    # Remplacez ces hyperparamètres par ceux trouvés par votre étude Optuna\n",
    "    best_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1,\n",
    "        'seed': 42,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 2000, # Garder un nombre élevé pour l'early stopping\n",
    "        # Collez ici les meilleurs paramètres d'Optuna\n",
    "        'learning_rate': 0.010241101044512771, # Exemple, à remplacer\n",
    "        'num_leaves': 204,      # Exemple, à remplacer\n",
    "        'max_depth': 12,        # Exemple, à remplacer\n",
    "        'min_child_samples': 200,# Exemple, à remplacer\n",
    "        'subsample': 0.8843947846459445,      # Exemple, à remplacer\n",
    "        'colsample_bytree': 0.7174115065647507, # Exemple, à remplacer\n",
    "    }\n",
    "\n",
    "    # Création du pipeline final\n",
    "    final_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('classifier', lgb.LGBMClassifier(**best_params))\n",
    "    ])\n",
    "    \n",
    "    X_train_part, X_val_part, y_train_part, y_val_part = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "    \n",
    "    final_pipeline.fit(\n",
    "        X_train_part, \n",
    "        y_train_part,\n",
    "        classifier__callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "        classifier__eval_set=[(X_val_part, y_val_part)]\n",
    "    )\n",
    "    print(\"Modèle final entraîné.\")\n",
    "\n",
    "    # --- SECTION 3 : ANALYSE DE L'IMPORTANCE DES FEATURES ---\n",
    "    print(\"\\n--- 3. Analyse de l'Importance des Features ---\")\n",
    "    EXPERIMENT_NAME = \"Credit Scoring - Final Model Analysis\"\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Final_LGBM_Optimized\"):\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # 3.1 Importance Globale\n",
    "        print(\"  - Calcul de l'importance globale des features...\")\n",
    "        feature_importances = final_pipeline.named_steps['classifier'].feature_importances_\n",
    "        feature_names = X.columns\n",
    "        importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances}).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(x='importance', y='feature', data=importance_df.head(20))\n",
    "        plt.title('Top 20 des Features les plus importantes (Global)')\n",
    "        plt.tight_layout()\n",
    "        global_importance_path = \"global_feature_importance.png\"\n",
    "        plt.savefig(global_importance_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(global_importance_path)\n",
    "        print(\"  - Graphique de l'importance globale loggué comme artefact.\")\n",
    "\n",
    "        # 3.2 Importance Locale avec SHAP\n",
    "        print(\"  - Calcul de l'importance locale (SHAP)...\")\n",
    "        # SHAP a besoin des données imputées\n",
    "        X_test_imputed = final_pipeline.named_steps['imputer'].transform(X_test)\n",
    "        \n",
    "        explainer = shap.TreeExplainer(final_pipeline.named_steps['classifier'])\n",
    "        shap_values = explainer.shap_values(X_test_imputed)\n",
    "\n",
    "        is_list_of_arrays = isinstance(shap_values, list)\n",
    "        \n",
    "        shap_values_for_plot = shap_values[1] if is_list_of_arrays else shap_values\n",
    "        expected_value_for_plot = explainer.expected_value[1] if is_list_of_arrays else explainer.expected_value\n",
    "\n",
    "        # Graphique résumé SHAP (beeswarm)\n",
    "        shap.summary_plot(shap_values_for_plot, X_test, show=False)\n",
    "        plt.title(\"Résumé SHAP de l'impact des features\")\n",
    "        plt.tight_layout()\n",
    "        shap_summary_path = \"shap_summary_plot.png\"\n",
    "        plt.savefig(shap_summary_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(shap_summary_path)\n",
    "        print(\"  - Graphique résumé SHAP loggué comme artefact.\")\n",
    "        \n",
    "        # MODIFIÉ : Remplacement du force_plot par des waterfall_plots pour 3 clients aléatoires\n",
    "        print(\"  - Création des graphiques waterfall SHAP pour 3 clients aléatoires...\")\n",
    "        \n",
    "        # Création de l'objet Explanation pour faciliter la manipulation\n",
    "        explanation = shap.Explanation(\n",
    "            values=shap_values_for_plot,\n",
    "            base_values=expected_value_for_plot,\n",
    "            data=X_test.values,\n",
    "            feature_names=X_test.columns\n",
    "        )\n",
    "\n",
    "        # Sélectionner 3 indices aléatoires\n",
    "        random_indices = X_test.sample(n=3, random_state=42).index\n",
    "\n",
    "        for idx in random_indices:\n",
    "            # Créer le waterfall plot pour une seule observation\n",
    "            shap.waterfall_plot(explanation[idx], max_display=15, show=False)\n",
    "            \n",
    "            client_id = test_ids[idx]\n",
    "            plt.title(f'SHAP Waterfall Plot pour le client {client_id}')\n",
    "            plt.tight_layout()\n",
    "            waterfall_path = f\"shap_waterfall_plot_client_{client_id}.png\"\n",
    "            plt.savefig(waterfall_path)\n",
    "            plt.close()\n",
    "            \n",
    "            # Logger l'artefact dans MLflow\n",
    "            mlflow.log_artifact(waterfall_path)\n",
    "            print(f\"    - Waterfall plot pour le client {client_id} loggué comme artefact.\")\n",
    "        \n",
    "        # --- SECTION 4 : ENREGISTREMENT ET SERVING DU MODÈLE ---\n",
    "        print(\"\\n--- 4. Enregistrement du Modèle dans le Model Registry ---\")\n",
    "        \n",
    "        registered_model_name = \"CreditScoringModel\"\n",
    "        \n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=final_pipeline,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=registered_model_name\n",
    "        )\n",
    "        print(f\"Modèle enregistré dans le Model Registry sous le nom : '{registered_model_name}'\")\n",
    "\n",
    "        # --- Test du \"Serving\" ---\n",
    "        print(\"\\n--- Test du Serving en chargeant le modèle depuis le registre ---\")\n",
    "        \n",
    "        try:\n",
    "            loaded_model = mlflow.pyfunc.load_model(f\"models:/{registered_model_name}/latest\")\n",
    "            print(\"Modèle chargé avec succès depuis le registre.\")\n",
    "\n",
    "            # On prend un des clients déjà expliqués\n",
    "            sample = X_test.loc[[random_indices[0]]]\n",
    "            prediction = loaded_model.predict(sample)\n",
    "            print(f\"\\nPrédiction test pour le client {test_ids.loc[random_indices[0]]}:\")\n",
    "            if isinstance(prediction, np.ndarray) and prediction.ndim == 2:\n",
    "                 print(f\"  Probabilité de défaut (classe 1) : {prediction[0][1]:.4f}\")\n",
    "            else:\n",
    "                 print(f\"  Probabilité (classe 1) : {prediction[0]:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERREUR lors du chargement ou de la prédiction avec le modèle du registre : {e}\")\n",
    "\n",
    "    print(\"\\n--- Processus Final Terminé ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
